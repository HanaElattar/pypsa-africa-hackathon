{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Use This Notebook\n",
    "\n",
    "1. Copy osm_pbf_data_extractor.py into the first cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script does the following\n",
    "# 1. Downloads OSM files for specified countries from Geofabrik\n",
    "# 2. Filters files for substations and lines\n",
    "# 3. Process and clean data\n",
    "# 4. Exports to CSV\n",
    "# 5. Exports to GeoJson\n",
    "\n",
    "\"\"\"\n",
    "OSM extraction scrpt\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# IMPORTANT: RUN SCRIPT FROM THIS SCRIPTS DIRECTORY i.e data_exploration/ TODO: make more robust\n",
    "# os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "sys.path.append(\"../../scripts\")\n",
    "\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from esy.osmfilter import run_filter\n",
    "from esy.osmfilter import Node, Relation, Way\n",
    "from esy.osmfilter import osm_info as osm_info\n",
    "from esy.osmfilter import osm_pickle as osm_pickle\n",
    "from iso_country_codes import AFRICA_CC\n",
    "from shapely.geometry import LineString, Point\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# https://gitlab.com/dlr-ve-esy/esy-osmfilter/-/tree/master/\n",
    "\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig()\n",
    "# logger=logging.getLogger(__name__)\n",
    "# logger.setLevel(logging.INFO)\n",
    "# logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Downloads PBF File for given Country Code\n",
    "\n",
    "\n",
    "def download_pbf(country_code, update):\n",
    "    \"\"\"\n",
    "    Downloads the pbf file from geofabrik for a given country code (see scripts/iso_country_codes.py).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    country_code : str\n",
    "    update : bool\n",
    "        name of the network component\n",
    "        update = true forces re-download of files\n",
    "    \"\"\"\n",
    "    country_name = AFRICA_CC[country_code]\n",
    "    # Filename for geofabrik\n",
    "    geofabrik_filename = f\"{country_name}-latest.osm.pbf\"\n",
    "    # https://download.geofabrik.de/africa/nigeria-latest.osm.pbf\n",
    "    geofabrik_url = f\"https://download.geofabrik.de/africa/{geofabrik_filename}\"\n",
    "    PBF_inputfile = os.path.join(\n",
    "        os.getcwd(), \"data\", \"osm\", \"pbf\", geofabrik_filename\n",
    "    )  # Input filepath\n",
    "\n",
    "    if not os.path.exists(PBF_inputfile) or update is True:\n",
    "        print(f\"{geofabrik_filename} does not exist, downloading to {PBF_inputfile}\")\n",
    "        #  create data/osm directory\n",
    "        os.makedirs(os.path.dirname(PBF_inputfile), exist_ok=True)\n",
    "        with requests.get(geofabrik_url, stream=True) as r:\n",
    "            with open(PBF_inputfile, \"wb\") as f:\n",
    "                shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "    return PBF_inputfile\n",
    "\n",
    "\n",
    "def download_and_filter(country_code, update=False):\n",
    "    PBF_inputfile = download_pbf(country_code, update)\n",
    "\n",
    "    filter_file_exists = False\n",
    "    # json file for the Data dictionary\n",
    "    JSON_outputfile = os.path.join(\n",
    "        os.getcwd(), \"data\", \"osm\", country_code + \"_power.json\"\n",
    "    )  # json file for the Elements dictionary is automatically written to \"data/osm/Elements\"+filename)\n",
    "\n",
    "    if os.path.exists(JSON_outputfile):\n",
    "        filter_file_exists = True\n",
    "\n",
    "    # Load Previously Pre-Filtered Files\n",
    "    if update is False and filter_file_exists is True:\n",
    "        create_elements = False  # Do not create elements again\n",
    "        new_prefilter_data = False  # Do not pre-filter data again\n",
    "        # HACKY: esy.osmfilter code to re-create Data.pickle\n",
    "        Data = osm_info.ReadJason(JSON_outputfile, verbose=\"no\")\n",
    "        DataDict = {\"Data\": Data}\n",
    "        osm_pickle.picklesave(\n",
    "            DataDict,\n",
    "            os.path.realpath(\n",
    "                os.path.join(os.getcwd(), os.path.dirname(JSON_outputfile))\n",
    "            ),\n",
    "        )\n",
    "        print(f\"Loading Pickle for {AFRICA_CC[country_code]}\")  # TODO: Change to Logger\n",
    "    else:\n",
    "        create_elements = True\n",
    "        new_prefilter_data = True\n",
    "        print(\n",
    "            f\"Creating  New Elements for {AFRICA_CC[country_code]}\"\n",
    "        )  # TODO: Change to Logger\n",
    "\n",
    "    prefilter = {\n",
    "        Node: {\"power\": [\"substation\", \"line\", \"generator\"]},\n",
    "        Way: {\"power\": [\"substation\", \"line\", \"generator\"]},\n",
    "        Relation: {\"power\": [\"substation\", \"line\", \"generator\"]},\n",
    "    }  # see https://dlr-ve-esy.gitlab.io/esy-osmfilter/filter.html for filter structures\n",
    "    # HACKY: due to esy.osmfilter validation\n",
    "\n",
    "    blackfilter = [\n",
    "        (\"\", \"\"),\n",
    "    ]\n",
    "\n",
    "    for feature in [\"substation\", \"line\", \"generator\"]:\n",
    "        whitefilter = [\n",
    "            [\n",
    "                (\"power\", feature),\n",
    "            ],\n",
    "        ]\n",
    "        elementname = f\"{country_code}_{feature}s\"\n",
    "\n",
    "        feature_data = run_filter(\n",
    "            elementname,\n",
    "            PBF_inputfile,\n",
    "            JSON_outputfile,\n",
    "            prefilter,\n",
    "            whitefilter,\n",
    "            blackfilter,\n",
    "            NewPreFilterData=new_prefilter_data,\n",
    "            CreateElements=create_elements,\n",
    "            LoadElements=True,\n",
    "            verbose=False,\n",
    "            multiprocess=True,\n",
    "        )\n",
    "\n",
    "        if feature == \"substation\":\n",
    "            substation_data = feature_data\n",
    "        if feature == \"line\":\n",
    "            line_data = feature_data\n",
    "        if feature == \"generator\":\n",
    "            generator_data = feature_data\n",
    "\n",
    "    return (substation_data, line_data, generator_data)\n",
    "\n",
    "\n",
    "# Convert Ways to Point Coordinates\n",
    "\n",
    "\n",
    "# TODO: Use shapely and merge with convert_ways_lines\n",
    "def convert_ways_nodes(df_way, Data):\n",
    "    lonlat_column = []\n",
    "    col = \"refs\"\n",
    "    df_way[col] = (\n",
    "        pd.Series().astype(float) if col not in df_way.columns else df_way[col]\n",
    "    )  # create empty \"refs\" if not in dataframe\n",
    "    for ref in df_way[\"refs\"]:\n",
    "        lonlats = []\n",
    "        for r in ref:\n",
    "            lonlat = Data[\"Node\"][str(r)][\"lonlat\"]\n",
    "            lonlats.append(lonlat)\n",
    "        lonlats = np.array(lonlats)\n",
    "        lonlat = np.mean(lonlats, axis=0)  # Hacky Apporx Centroid\n",
    "        lonlat_column.append(lonlat)\n",
    "    df_way.drop(\"refs\", axis=1, inplace=True, errors=\"ignore\")\n",
    "    df_way.insert(0, \"lonlat\", lonlat_column)\n",
    "\n",
    "\n",
    "# Convert Ways to Line Coordinates\n",
    "\n",
    "\n",
    "def convert_ways_lines(df_way, Data):\n",
    "    lonlat_column = []\n",
    "    for ref in df_way[\"refs\"]:  # goes through each row in df_way[\"refs\"]\n",
    "        lonlats = []\n",
    "        # picks each element in ref & replaces ID by coordinate tuple (A multiline consist of several points)\n",
    "        for r in ref:\n",
    "            # \"r\" is the ID in Data[\"Node\"], [\"lonlat\"] a list of [x1,y1] (coordinates)\n",
    "            lonlat = Data[\"Node\"][str(r)][\"lonlat\"]\n",
    "            lonlat = tuple(lonlat)\n",
    "            lonlats.append(lonlat)  # a list with tuples\n",
    "        lonlat_column.append(lonlats)  # adding a new list of tuples every row\n",
    "    df_way.drop(\"refs\", axis=1, inplace=True)\n",
    "    df_way.insert(1, \"lonlat\", lonlat_column)\n",
    "\n",
    "\n",
    "# Convert Points Pandas Dataframe to GeoPandas Dataframe\n",
    "\n",
    "\n",
    "def convert_pd_to_gdf(df_way):\n",
    "    gdf = gpd.GeoDataFrame(df_way, geometry=[Point(x, y) for x, y in df_way.lonlat], crs=\"EPSG:4326\")\n",
    "    gdf.drop(columns=[\"lonlat\"], inplace=True)\n",
    "    return gdf\n",
    "\n",
    "\n",
    "# Convert Lines Pandas Dataframe to GeoPandas Dataframe\n",
    "\n",
    "\n",
    "def convert_pd_to_gdf_lines(df_way, simplified=False):\n",
    "    df_way[\"geometry\"] = df_way[\"lonlat\"].apply(lambda x: LineString(x))\n",
    "    if simplified is True:\n",
    "        df_way[\"geometry\"] = df_way[\"geometry\"].apply(\n",
    "            lambda x: x.simplify(0.005, preserve_topology=False)\n",
    "        )\n",
    "    gdf = gpd.GeoDataFrame(df_way, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "    gdf.drop(columns=[\"lonlat\"], inplace=True)\n",
    "\n",
    "    return gdf\n",
    "\n",
    "\n",
    "# Convert Filtered Data, Elements to Pandas Dataframes\n",
    "\n",
    "\n",
    "def convert_filtered_data_to_dfs(country_code, feature_data, feature):\n",
    "    [Data, Elements] = feature_data\n",
    "    elementname = f\"{country_code}_{feature}s\"\n",
    "    df_way = pd.json_normalize(Elements[elementname][\"Way\"].values())\n",
    "    df_node = pd.json_normalize(Elements[elementname][\"Node\"].values())\n",
    "    return (df_node, df_way, Data)\n",
    "\n",
    "\n",
    "def process_substation_data(country_code, substation_data):\n",
    "    df_node, df_way, Data = convert_filtered_data_to_dfs(\n",
    "        country_code, substation_data, \"substation\"\n",
    "    )\n",
    "    convert_ways_nodes(df_way, Data)\n",
    "    # Add Type Column\n",
    "    df_node[\"Type\"] = \"Node\"\n",
    "    df_way[\"Type\"] = \"Way\"\n",
    "\n",
    "    df_combined = pd.concat([df_node, df_way], axis=0)\n",
    "    # Add Country Column\n",
    "    df_combined[\"Country\"] = AFRICA_CC[country_code]\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "\n",
    "def process_line_data(country_code, line_data):\n",
    "    df_node, df_way, Data = convert_filtered_data_to_dfs(\n",
    "        country_code, line_data, \"line\"\n",
    "    )\n",
    "    convert_ways_lines(df_way, Data)\n",
    "    # Add Type Column\n",
    "    df_way[\"Type\"] = \"Way\"\n",
    "\n",
    "    # Add Country Column\n",
    "    df_way[\"Country\"] = AFRICA_CC[country_code]\n",
    "    return df_way\n",
    "\n",
    "\n",
    "def process_generator_data(country_code, generator_data):\n",
    "    df_node, df_way, Data = convert_filtered_data_to_dfs(\n",
    "        country_code, generator_data, \"generator\"\n",
    "    )\n",
    "    convert_ways_nodes(df_way, Data)\n",
    "    # Add Type Column\n",
    "    df_node[\"Type\"] = \"Node\"\n",
    "    df_way[\"Type\"] = \"Way\"\n",
    "\n",
    "    df_combined = pd.concat([df_node, df_way], axis=0)\n",
    "    # Add Country Column\n",
    "    df_combined[\"Country\"] = AFRICA_CC[country_code]\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "\n",
    "def process_data():\n",
    "    df_all_substations = pd.DataFrame()\n",
    "    df_all_lines = pd.DataFrame()\n",
    "    df_all_generators = pd.DataFrame()\n",
    "    test_CC = {\"NG\": \"nigeria\"}\n",
    "    for country_code in test_CC.keys():\n",
    "        substation_data, line_data, generator_data = download_and_filter(country_code)\n",
    "        for feature in [\"substation\", \"line\", \"generator\"]:\n",
    "            if feature == \"substation\":\n",
    "                df_substation = process_substation_data(country_code, substation_data)\n",
    "                df_all_substations = pd.concat([df_all_substations, df_substation])\n",
    "            if feature == \"line\":\n",
    "                df_line = process_line_data(country_code, line_data)\n",
    "                df_all_lines = pd.concat([df_all_lines, df_line])\n",
    "            if feature == \"generator\":\n",
    "                df_generator = process_generator_data(country_code, generator_data)\n",
    "                df_all_generators = pd.concat([df_all_generators, df_generator])\n",
    "\n",
    "    # ----------- SUBSTATIONS -----------\n",
    "\n",
    "    # Columns of interest\n",
    "    df_all_substations = df_all_substations[\n",
    "        df_all_substations.columns &\n",
    "        [\n",
    "            \"id\",\n",
    "            \"lonlat\",\n",
    "            \"tags.power\",\n",
    "            \"tags.substation\",\n",
    "            \"tags.voltage\",\n",
    "            \"tags.frequency\",\n",
    "            \"Type\",\n",
    "            \"Country\",\n",
    "        ]\n",
    "    ]\n",
    "    df_all_substations.drop(df_all_substations.loc[df_all_substations['tags.substation']=='industrial'].index, inplace=True) # Drop industrial substations\n",
    "    df_all_substations.drop(df_all_substations.loc[df_all_substations['tags.substation']=='distribution'].index, inplace=True) # Drop distribution substations\n",
    "\n",
    "    # Generate Files\n",
    "    outputfile_partial = os.path.join(\n",
    "        os.getcwd(), \"data\", \"africa_all\" + \"_substations.\"\n",
    "    )\n",
    "    df_all_substations.to_csv(outputfile_partial + \"csv\")  # Generate CSV\n",
    "    gdf_substations = convert_pd_to_gdf(df_all_substations)\n",
    "    gdf_substations.to_file(\n",
    "        outputfile_partial + \"geojson\", driver=\"GeoJSON\"\n",
    "    )  # Generate GeoJson\n",
    "\n",
    "    # ----------- LINES -----------\n",
    "\n",
    "    # Columns of interest\n",
    "    df_all_lines = df_all_lines[\n",
    "        df_all_lines.columns &\n",
    "        [\n",
    "            \"id\",\n",
    "            \"lonlat\",\n",
    "            \"tags.power\",\n",
    "            \"tags.cables\",\n",
    "            \"tags.voltage\",\n",
    "            \"tags.circuits\",\n",
    "            \"tags.frequency\",\n",
    "            \"Type\",\n",
    "            \"Country\",\n",
    "        ]\n",
    "    ]\n",
    "    # Generate Files\n",
    "    outputfile_partial = os.path.join(os.getcwd(), \"data\", \"africa_all\" + \"_lines.\")\n",
    "    df_all_lines.to_csv(outputfile_partial + \"csv\")  # Generate CSV\n",
    "    gdf_lines = convert_pd_to_gdf_lines(df_all_lines, simplified=True)\n",
    "    gdf_lines.to_file(\n",
    "        outputfile_partial + \"geojson\", driver=\"GeoJSON\"\n",
    "    )  # Generate GeoJson\n",
    "\n",
    "    # ----------- Generator -----------\n",
    "\n",
    "    # Columns of interest\n",
    "    df_all_generators = df_all_generators[\n",
    "        df_all_generators.columns &\n",
    "        [\n",
    "            \"id\",\n",
    "            \"lonlat\",\n",
    "            \"tags.power\",\n",
    "            \"tags.generator:type\",\n",
    "            \"tags.generator:method\",\n",
    "            \"tags.generator:source\",\n",
    "            \"tags.generator:output:electricity\",\n",
    "            \"Type\",\n",
    "            \"Country\",\n",
    "        ]\n",
    "    ]\n",
    "    # Generate Files\n",
    "    outputfile_partial = os.path.join(\n",
    "        os.getcwd(), \"data\", \"africa_all\" + \"_generators.\"\n",
    "    )\n",
    "    df_all_generators.to_csv(outputfile_partial + \"csv\")  # Generate CSV\n",
    "    gdf_generators = convert_pd_to_gdf(df_all_generators)\n",
    "    gdf_generators.to_file(\n",
    "        outputfile_partial + \"geojson\", driver=\"GeoJSON\"\n",
    "    )  # Generate GeoJson\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overwrite Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New function\n",
    "\n",
    "def lonlat_lookup(df_way, Data):\n",
    "    lonlat_list = []\n",
    "\n",
    "    col = \"refs\"\n",
    "    if col not in df_way.columns:\n",
    "        print (\"refs column not found\")\n",
    "        df_way[col] = pd.Series().astype(float) # create empty \"refs\" if not in dataframe\n",
    "      \n",
    "    for ref in df_way[\"refs\"]:\n",
    "        lonlat_row = []\n",
    "        for r in ref:\n",
    "            lonlat = tuple(Data[\"Node\"][str(r)][\"lonlat\"])\n",
    "            lonlat_row.append(lonlat)\n",
    "        lonlat_list.append(lonlat_row)\n",
    "    return lonlat_list\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "def convert_ways_point(df_way, Data):\n",
    "    lonlat_list = lonlat_lookup(df_way, Data)\n",
    "    lonlat_column = []\n",
    "    area_column = []\n",
    "    for lonlat in lonlat_list:\n",
    "        way_polygon = Polygon(lonlat)\n",
    "        polygon_area = int(round(gpd.GeoSeries(way_polygon).set_crs(\"EPSG:4326\").to_crs(\"EPSG:3857\").area, -1)) # nearest tens\n",
    "        # print('{:g}'.format(float('{:.3g}'.format(float(polygon_area))))) # For significant numbers\n",
    "        area_column.append(polygon_area)\n",
    "        center_point = way_polygon.centroid\n",
    "        lonlat_column.append(list((center_point.x, center_point.y)))\n",
    "\n",
    "    # df_way.drop(\"refs\", axis=1, inplace=True, errors=\"ignore\")\n",
    "    df_way.insert(0, \"lonlat\", lonlat_column)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_substations = pd.DataFrame()\n",
    "df_all_lines = pd.DataFrame()\n",
    "df_all_generators = pd.DataFrame()\n",
    "test_CC = {\"NG\": \"nigeria\"}\n",
    "for country_code in test_CC.keys():\n",
    "    substation_data, line_data, generator_data = download_and_filter(country_code)\n",
    "    for feature in [\"substation\", \"line\",\"generator\"]:\n",
    "        if feature == 'substation':\n",
    "            df_substation = process_substation_data(country_code, substation_data)\n",
    "            df_all_substations = pd.concat(\n",
    "                [df_all_substations, df_substation])\n",
    "        if feature == 'line':\n",
    "            df_line = process_line_data(country_code, line_data)\n",
    "            df_all_lines = pd.concat([df_all_lines, df_line])\n",
    "        if feature == 'generator':\n",
    "            df_generator = process_generator_data(country_code, generator_data)\n",
    "            df_all_generators = pd.concat(\n",
    "                [df_all_generators, df_generator])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Substations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- SUBSTATIONS -----------\n",
    "\n",
    "# Clean\n",
    "df_all_substations.reset_index(drop=True, inplace=True)\n",
    "df_all_substations.dropna(thresh=len(df_all_substations)*0.25, axis=1, how='all', inplace = True) #Drop Columns with 75% values as N/A\n",
    "df_all_substations.dropna(subset=['tags.voltage'], inplace = True) # Drop any substations with Voltage = N/A\n",
    "df_all_substations.drop(df_all_substations.loc[df_all_substations['tags.substation']=='industrial'].index, inplace=True)\n",
    "df_all_substations.drop(df_all_substations.loc[df_all_substations['tags.substation']=='distribution'].index, inplace=True)\n",
    "\n",
    "# Generate Files\n",
    "outputfile_partial = os.path.join(os.getcwd(),'data','africa_all'+'_substations.')\n",
    "df_all_substations.to_csv(outputfile_partial + 'csv') # Generate CSV\n",
    "gdf_substations = convert_pd_to_gdf(df_all_substations.drop('refs', 1))\n",
    "gdf_substations.to_file(outputfile_partial+'geojson', driver=\"GeoJSON\")  # Generate GeoJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_substations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Generator -----------\n",
    "\n",
    "df_all_generators.reset_index(drop=True, inplace=True)\n",
    "df_all_generators.drop(columns = [\"tags.fixme\",\"tags.frequency\",\"tags.name:ar\",\"tags.building\",\"tags.barrier\"], inplace = True, errors='ignore')\n",
    "df_all_generators = df_all_generators[df_all_generators['tags.generator:output:electricity'].astype(str).str.contains('MW')] #removes boolean \n",
    "df_all_generators['tags.generator:output:electricity'] = df_all_generators['tags.generator:output:electricity'].str.extract('(\\d+)').astype(float)\n",
    "df_all_generators.rename(columns = {'tags.generator:output:electricity':\"power_output_MW\"}, inplace = True)\n",
    "# df_all_generators.dropna(thresh=len(df_all_generators)*0.25, axis=1, how='all', inplace=True) # Drop Columns with 75% values as N/A\n",
    "\n",
    "# # Generate Files\n",
    "# outputfile_partial = os.path.join(os.getcwd(),'data','africa_all'+'_generators.')\n",
    "# df_all_generators.to_csv(outputfile_partial + 'csv') # Generate CSV\n",
    "# gdf_generators = convert_pd_to_gdf(df_all_generators)\n",
    "# gdf_generators.to_file(outputfile_partial+'geojson', driver=\"GeoJSON\")  # Generate GeoJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- LINES -----------\n",
    "\n",
    "# Clean\n",
    "# TODO: FIX Voltage Filter\n",
    "# Some transmission lines carry multiple voltages, having voltage_V = 10000;20000  (two lines)\n",
    "# The following code keeps only the first information before the semicolon..\n",
    "# Needs to be corrected in future, creating two lines with the same bus ID.\n",
    "\n",
    "\n",
    "df_all_lines = df_all_lines[\n",
    "        {\n",
    "            \"id\",\n",
    "            \"refs\",\n",
    "            \"lonlat\",\n",
    "            \"tags.power\",\n",
    "            \"tags.cables\",\n",
    "            \"tags.voltage\",\n",
    "            \"tags.frequency\",\n",
    "            \"Type\",\n",
    "            \"Country\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Clean data    \n",
    "df_all_lines = df_all_lines.reset_index(drop=True)\n",
    "df_all_lines = df_all_lines.dropna(subset=['tags.voltage']) # Drop any lines with Voltage = N/A\n",
    "df_all_lines = df_all_lines.rename(columns = {'tags.voltage':\"voltage_V\"}) \n",
    "df_all_lines['voltage_V'] = df_all_lines['voltage_V'].str.split('*').str[0] #just keeps the \n",
    "df_all_lines['voltage_V'] = df_all_lines['voltage_V'].str.split(';').str[0]\n",
    "df_all_lines['voltage_V'] = df_all_lines['voltage_V'].apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna() ## if cell can't converted to float -> drop\n",
    "df_all_lines = df_all_lines[df_all_lines.voltage_V > 10000]\n",
    "# df_all_lines['end_refs'] = \n",
    "\n",
    "# Generate Files\n",
    "outputfile_partial = os.path.join(os.getcwd(), 'data', 'africa_all'+'_lines.')  \n",
    "df_all_lines.to_csv(outputfile_partial + 'csv')  # Generate CSV\n",
    "gdf_lines = convert_pd_to_gdf_lines(df_all_lines.drop('refs', 1), simplified=True)\n",
    "gdf_lines.to_file(outputfile_partial+'geojson',\n",
    "            driver=\"GeoJSON\")  # Generate GeoJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_line_lookup = df_all_lines[['id','refs']]\n",
    "display(df_line_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_node_lookup = df_all_substations[['id','refs']]\n",
    "df_node_lookup = df_node_lookup.dropna(subset=['refs']) # Drop any nodes with refs = N/A\n",
    "display(df_node_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_df(df):\n",
    "    out = []\n",
    "    for n, row in df.iterrows():\n",
    "        for item in row['refs']:\n",
    "            row['flat_ref'] = item\n",
    "            out += [row.copy()]\n",
    "\n",
    "    flattened_df = pd.DataFrame(out)\n",
    "    flattened_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return flattened_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "way_f = flatten_df(df_line_lookup)\n",
    "display(way_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_f = flatten_df(df_node_lookup)\n",
    "display(node_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = way_f.merge(node_f, on='flat_ref', how='inner')\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_line_refs = set()\n",
    "for ref in df_all_lines[\"refs\"]:  # goes through each row in df_way['refs']\n",
    "    for r in ref:\n",
    "        all_line_refs.add(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_st_refs = set()\n",
    "for ref in df_all_substations.dropna(subset=['refs'])[\"refs\"]:  # goes through each row in df_way['refs']\n",
    "    for r in ref:\n",
    "        all_st_refs.add(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_line_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_line_refs.intersection(all_st_refs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pypsa': conda)",
   "name": "python3710jvsc74a57bd09a6789bfb301b77bab315636af11818455845432a243e82ed991667bc6cd3910"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
