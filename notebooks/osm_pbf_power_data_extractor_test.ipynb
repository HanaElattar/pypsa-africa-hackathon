{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Use This Notebook\n",
    "\n",
    "1. Copy osm_pbf_data_extractor.py into the first cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script does the following\r\n",
    "# 1. Downloads OSM files for specified countries from Geofabrik\r\n",
    "# 2. Filters files for substations and lines\r\n",
    "# 3. Process and clean data\r\n",
    "# 4. Exports to CSV\r\n",
    "# 5. Exports to GeoJson\r\n",
    "\r\n",
    "\"\"\"\r\n",
    "OSM extraction scrpt\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "import os\r\n",
    "import sys\r\n",
    "\r\n",
    "# IMPORTANT: RUN SCRIPT FROM THIS SCRIPTS DIRECTORY i.e data_exploration/ TODO: make more robust\r\n",
    "# os.chdir(os.path.dirname(os.path.abspath(__file__)))\r\n",
    "sys.path.append('../')  # to import helpers\r\n",
    "\r\n",
    "import logging\r\n",
    "import shutil\r\n",
    "\r\n",
    "import geopandas as gpd\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import requests\r\n",
    "from esy.osmfilter import run_filter\r\n",
    "from esy.osmfilter import Node, Relation, Way\r\n",
    "from esy.osmfilter import osm_info as osm_info\r\n",
    "from esy.osmfilter import osm_pickle as osm_pickle\r\n",
    "from scripts.osm_data_config import world_countries\r\n",
    "from shapely.geometry import LineString, Point\r\n",
    "\r\n",
    "logger = logging.getLogger(__name__)\r\n",
    "\r\n",
    "# https://gitlab.com/dlr-ve-esy/esy-osmfilter/-/tree/master/\r\n",
    "\r\n",
    "\r\n",
    "# import logging\r\n",
    "# logging.basicConfig()\r\n",
    "# logger=logging.getLogger(__name__)\r\n",
    "# logger.setLevel(logging.INFO)\r\n",
    "# logger.setLevel(logging.WARNING)\r\n",
    "\r\n",
    "# Downloads PBF File for given Country Code\r\n",
    "\r\n",
    "\r\n",
    "def download_pbf(country_code, update):\r\n",
    "    \"\"\"\r\n",
    "    Downloads the pbf file from geofabrik for a given country code (see scripts/osm_data_config.py).\r\n",
    "\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    country_code : str\r\n",
    "    update : bool\r\n",
    "        name of the network component\r\n",
    "        update = true forces re-download of files\r\n",
    "    \"\"\"\r\n",
    "    country_name = AFRICA_CC[country_code]\r\n",
    "    # Filename for geofabrik\r\n",
    "    geofabrik_filename = f\"{country_name}-latest.osm.pbf\"\r\n",
    "    # https://download.geofabrik.de/africa/nigeria-latest.osm.pbf\r\n",
    "    geofabrik_url = f\"https://download.geofabrik.de/africa/{geofabrik_filename}\"\r\n",
    "    PBF_inputfile = os.path.join(\r\n",
    "        os.getcwd(), \"data\", \"osm\", \"pbf\", geofabrik_filename\r\n",
    "    )  # Input filepath\r\n",
    "\r\n",
    "    if not os.path.exists(PBF_inputfile) or update is True:\r\n",
    "        print(f\"{geofabrik_filename} does not exist, downloading to {PBF_inputfile}\")\r\n",
    "        #  create data/osm directory\r\n",
    "        os.makedirs(os.path.dirname(PBF_inputfile), exist_ok=True)\r\n",
    "        with requests.get(geofabrik_url, stream=True) as r:\r\n",
    "            with open(PBF_inputfile, \"wb\") as f:\r\n",
    "                shutil.copyfileobj(r.raw, f)\r\n",
    "\r\n",
    "    return PBF_inputfile\r\n",
    "\r\n",
    "\r\n",
    "def download_and_filter(country_code, update=False):\r\n",
    "    PBF_inputfile = download_pbf(country_code, update)\r\n",
    "\r\n",
    "    filter_file_exists = False\r\n",
    "    # json file for the Data dictionary\r\n",
    "    JSON_outputfile = os.path.join(\r\n",
    "        os.getcwd(), \"data\", \"osm\", country_code + \"_power.json\"\r\n",
    "    )  # json file for the Elements dictionary is automatically written to \"data/osm/Elements\"+filename)\r\n",
    "\r\n",
    "    if os.path.exists(JSON_outputfile):\r\n",
    "        filter_file_exists = True\r\n",
    "\r\n",
    "    # Load Previously Pre-Filtered Files\r\n",
    "    if update is False and filter_file_exists is True:\r\n",
    "        create_elements = False  # Do not create elements again\r\n",
    "        new_prefilter_data = False  # Do not pre-filter data again\r\n",
    "        # HACKY: esy.osmfilter code to re-create Data.pickle\r\n",
    "        Data = osm_info.ReadJason(JSON_outputfile, verbose=\"no\")\r\n",
    "        DataDict = {\"Data\": Data}\r\n",
    "        osm_pickle.picklesave(\r\n",
    "            DataDict,\r\n",
    "            os.path.realpath(\r\n",
    "                os.path.join(os.getcwd(), os.path.dirname(JSON_outputfile))\r\n",
    "            ),\r\n",
    "        )\r\n",
    "        print(f\"Loading Pickle for {world_countries[country_code]}\")  # TODO: Change to Logger\r\n",
    "    else:\r\n",
    "        create_elements = True\r\n",
    "        new_prefilter_data = True\r\n",
    "        print(\r\n",
    "            f\"Creating  New Elements for {world_countries[country_code]}\"\r\n",
    "        )  # TODO: Change to Logger\r\n",
    "\r\n",
    "    prefilter = {\r\n",
    "        Node: {\"power\": [\"substation\", \"line\", \"generator\"]},\r\n",
    "        Way: {\"power\": [\"substation\", \"line\", \"generator\"]},\r\n",
    "        Relation: {\"power\": [\"substation\", \"line\", \"generator\"]},\r\n",
    "    }  # see https://dlr-ve-esy.gitlab.io/esy-osmfilter/filter.html for filter structures\r\n",
    "    # HACKY: due to esy.osmfilter validation\r\n",
    "\r\n",
    "    blackfilter = [\r\n",
    "        (\"\", \"\"),\r\n",
    "    ]\r\n",
    "\r\n",
    "    for feature in [\"substation\", \"line\", \"generator\"]:\r\n",
    "        whitefilter = [\r\n",
    "            [\r\n",
    "                (\"power\", feature),\r\n",
    "            ],\r\n",
    "        ]\r\n",
    "        elementname = f\"{country_code}_{feature}s\"\r\n",
    "\r\n",
    "        feature_data = run_filter(\r\n",
    "            elementname,\r\n",
    "            PBF_inputfile,\r\n",
    "            JSON_outputfile,\r\n",
    "            prefilter,\r\n",
    "            whitefilter,\r\n",
    "            blackfilter,\r\n",
    "            NewPreFilterData=new_prefilter_data,\r\n",
    "            CreateElements=create_elements,\r\n",
    "            LoadElements=True,\r\n",
    "            verbose=False,\r\n",
    "            multiprocess=True,\r\n",
    "        )\r\n",
    "\r\n",
    "        if feature == \"substation\":\r\n",
    "            substation_data = feature_data\r\n",
    "        if feature == \"line\":\r\n",
    "            line_data = feature_data\r\n",
    "        if feature == \"generator\":\r\n",
    "            generator_data = feature_data\r\n",
    "\r\n",
    "    return (substation_data, line_data, generator_data)\r\n",
    "\r\n",
    "\r\n",
    "# Convert Ways to Point Coordinates\r\n",
    "\r\n",
    "\r\n",
    "# TODO: Use shapely and merge with convert_ways_lines\r\n",
    "def convert_ways_nodes(df_way, Data):\r\n",
    "    lonlat_column = []\r\n",
    "    col = \"refs\"\r\n",
    "    df_way[col] = (\r\n",
    "        pd.Series().astype(float) if col not in df_way.columns else df_way[col]\r\n",
    "    )  # create empty \"refs\" if not in dataframe\r\n",
    "    for ref in df_way[\"refs\"]:\r\n",
    "        lonlats = []\r\n",
    "        for r in ref:\r\n",
    "            lonlat = Data[\"Node\"][str(r)][\"lonlat\"]\r\n",
    "            lonlats.append(lonlat)\r\n",
    "        lonlats = np.array(lonlats)\r\n",
    "        lonlat = np.mean(lonlats, axis=0)  # Hacky Apporx Centroid\r\n",
    "        lonlat_column.append(lonlat)\r\n",
    "    df_way.drop(\"refs\", axis=1, inplace=True, errors=\"ignore\")\r\n",
    "    df_way.insert(0, \"lonlat\", lonlat_column)\r\n",
    "\r\n",
    "\r\n",
    "# Convert Ways to Line Coordinates\r\n",
    "\r\n",
    "\r\n",
    "def convert_ways_lines(df_way, Data):\r\n",
    "    lonlat_column = []\r\n",
    "    for ref in df_way[\"refs\"]:  # goes through each row in df_way[\"refs\"]\r\n",
    "        lonlats = []\r\n",
    "        # picks each element in ref & replaces ID by coordinate tuple (A multiline consist of several points)\r\n",
    "        for r in ref:\r\n",
    "            # \"r\" is the ID in Data[\"Node\"], [\"lonlat\"] a list of [x1,y1] (coordinates)\r\n",
    "            lonlat = Data[\"Node\"][str(r)][\"lonlat\"]\r\n",
    "            lonlat = tuple(lonlat)\r\n",
    "            lonlats.append(lonlat)  # a list with tuples\r\n",
    "        lonlat_column.append(lonlats)  # adding a new list of tuples every row\r\n",
    "    df_way.drop(\"refs\", axis=1, inplace=True)\r\n",
    "    df_way.insert(1, \"lonlat\", lonlat_column)\r\n",
    "\r\n",
    "\r\n",
    "# Convert Points Pandas Dataframe to GeoPandas Dataframe\r\n",
    "\r\n",
    "\r\n",
    "def convert_pd_to_gdf(df_way):\r\n",
    "    gdf = gpd.GeoDataFrame(df_way, geometry=[Point(x, y) for x, y in df_way.lonlat], crs=\"EPSG:4326\")\r\n",
    "    gdf.drop(columns=[\"lonlat\"], inplace=True)\r\n",
    "    return gdf\r\n",
    "\r\n",
    "\r\n",
    "# Convert Lines Pandas Dataframe to GeoPandas Dataframe\r\n",
    "\r\n",
    "\r\n",
    "def convert_pd_to_gdf_lines(df_way, simplified=False):\r\n",
    "    df_way[\"geometry\"] = df_way[\"lonlat\"].apply(lambda x: LineString(x))\r\n",
    "    if simplified is True:\r\n",
    "        df_way[\"geometry\"] = df_way[\"geometry\"].apply(\r\n",
    "            lambda x: x.simplify(0.005, preserve_topology=False)\r\n",
    "        )\r\n",
    "    gdf = gpd.GeoDataFrame(df_way, geometry=\"geometry\", crs=\"EPSG:4326\")\r\n",
    "    gdf.drop(columns=[\"lonlat\"], inplace=True)\r\n",
    "\r\n",
    "    return gdf\r\n",
    "\r\n",
    "\r\n",
    "# Convert Filtered Data, Elements to Pandas Dataframes\r\n",
    "\r\n",
    "\r\n",
    "def convert_filtered_data_to_dfs(country_code, feature_data, feature):\r\n",
    "    [Data, Elements] = feature_data\r\n",
    "    elementname = f\"{country_code}_{feature}s\"\r\n",
    "    df_way = pd.json_normalize(Elements[elementname][\"Way\"].values())\r\n",
    "    df_node = pd.json_normalize(Elements[elementname][\"Node\"].values())\r\n",
    "    return (df_node, df_way, Data)\r\n",
    "\r\n",
    "\r\n",
    "def process_substation_data(country_code, substation_data):\r\n",
    "    df_node, df_way, Data = convert_filtered_data_to_dfs(\r\n",
    "        country_code, substation_data, \"substation\"\r\n",
    "    )\r\n",
    "    convert_ways_nodes(df_way, Data)\r\n",
    "    # Add Type Column\r\n",
    "    df_node[\"Type\"] = \"Node\"\r\n",
    "    df_way[\"Type\"] = \"Way\"\r\n",
    "\r\n",
    "    df_combined = pd.concat([df_node, df_way], axis=0)\r\n",
    "    # Add Country Column\r\n",
    "    df_combined[\"Country\"] = world_countries[country_code]\r\n",
    "\r\n",
    "    return df_combined\r\n",
    "\r\n",
    "\r\n",
    "def process_line_data(country_code, line_data):\r\n",
    "    df_node, df_way, Data = convert_filtered_data_to_dfs(\r\n",
    "        country_code, line_data, \"line\"\r\n",
    "    )\r\n",
    "    convert_ways_lines(df_way, Data)\r\n",
    "    # Add Type Column\r\n",
    "    df_way[\"Type\"] = \"Way\"\r\n",
    "\r\n",
    "    # Add Country Column\r\n",
    "    df_way[\"Country\"] = world_countries[country_code]\r\n",
    "    return df_way\r\n",
    "\r\n",
    "\r\n",
    "def process_generator_data(country_code, generator_data):\r\n",
    "    df_node, df_way, Data = convert_filtered_data_to_dfs(\r\n",
    "        country_code, generator_data, \"generator\"\r\n",
    "    )\r\n",
    "    convert_ways_nodes(df_way, Data)\r\n",
    "    # Add Type Column\r\n",
    "    df_node[\"Type\"] = \"Node\"\r\n",
    "    df_way[\"Type\"] = \"Way\"\r\n",
    "\r\n",
    "    df_combined = pd.concat([df_node, df_way], axis=0)\r\n",
    "    # Add Country Column\r\n",
    "    df_combined[\"Country\"] = world_countries[country_code]\r\n",
    "\r\n",
    "    return df_combined\r\n",
    "\r\n",
    "\r\n",
    "def process_data():\r\n",
    "    df_all_substations = pd.DataFrame()\r\n",
    "    df_all_lines = pd.DataFrame()\r\n",
    "    df_all_generators = pd.DataFrame()\r\n",
    "    test_CC = {\"NG\": \"nigeria\"}\r\n",
    "    for country_code in test_CC.keys():\r\n",
    "        substation_data, line_data, generator_data = download_and_filter(country_code)\r\n",
    "        for feature in [\"substation\", \"line\", \"generator\"]:\r\n",
    "            if feature == \"substation\":\r\n",
    "                df_substation = process_substation_data(country_code, substation_data)\r\n",
    "                df_all_substations = pd.concat([df_all_substations, df_substation])\r\n",
    "            if feature == \"line\":\r\n",
    "                df_line = process_line_data(country_code, line_data)\r\n",
    "                df_all_lines = pd.concat([df_all_lines, df_line])\r\n",
    "            if feature == \"generator\":\r\n",
    "                df_generator = process_generator_data(country_code, generator_data)\r\n",
    "                df_all_generators = pd.concat([df_all_generators, df_generator])\r\n",
    "\r\n",
    "    # ----------- SUBSTATIONS -----------\r\n",
    "\r\n",
    "    # Columns of interest\r\n",
    "    df_all_substations = df_all_substations[\r\n",
    "        df_all_substations.columns &\r\n",
    "        [\r\n",
    "            \"id\",\r\n",
    "            \"lonlat\",\r\n",
    "            \"tags.power\",\r\n",
    "            \"tags.substation\",\r\n",
    "            \"tags.voltage\",\r\n",
    "            \"tags.frequency\",\r\n",
    "            \"Type\",\r\n",
    "            \"Country\",\r\n",
    "        ]\r\n",
    "    ]\r\n",
    "    df_all_substations.drop(df_all_substations.loc[df_all_substations['tags.substation']=='industrial'].index, inplace=True) # Drop industrial substations\r\n",
    "    df_all_substations.drop(df_all_substations.loc[df_all_substations['tags.substation']=='distribution'].index, inplace=True) # Drop distribution substations\r\n",
    "\r\n",
    "    # Generate Files\r\n",
    "    outputfile_partial = os.path.join(\r\n",
    "        os.getcwd(), \"data\", \"africa_all\" + \"_substations.\"\r\n",
    "    )\r\n",
    "    df_all_substations.to_csv(outputfile_partial + \"csv\")  # Generate CSV\r\n",
    "    gdf_substations = convert_pd_to_gdf(df_all_substations)\r\n",
    "    gdf_substations.to_file(\r\n",
    "        outputfile_partial + \"geojson\", driver=\"GeoJSON\"\r\n",
    "    )  # Generate GeoJson\r\n",
    "\r\n",
    "    # ----------- LINES -----------\r\n",
    "\r\n",
    "    # Columns of interest\r\n",
    "    df_all_lines = df_all_lines[\r\n",
    "        df_all_lines.columns &\r\n",
    "        [\r\n",
    "            \"id\",\r\n",
    "            \"lonlat\",\r\n",
    "            \"tags.power\",\r\n",
    "            \"tags.cables\",\r\n",
    "            \"tags.voltage\",\r\n",
    "            \"tags.circuits\",\r\n",
    "            \"tags.frequency\",\r\n",
    "            \"Type\",\r\n",
    "            \"Country\",\r\n",
    "        ]\r\n",
    "    ]\r\n",
    "    # Generate Files\r\n",
    "    outputfile_partial = os.path.join(os.getcwd(), \"data\", \"africa_all\" + \"_lines.\")\r\n",
    "    df_all_lines.to_csv(outputfile_partial + \"csv\")  # Generate CSV\r\n",
    "    gdf_lines = convert_pd_to_gdf_lines(df_all_lines, simplified=True)\r\n",
    "    gdf_lines.to_file(\r\n",
    "        outputfile_partial + \"geojson\", driver=\"GeoJSON\"\r\n",
    "    )  # Generate GeoJson\r\n",
    "\r\n",
    "    # ----------- Generator -----------\r\n",
    "\r\n",
    "    # Columns of interest\r\n",
    "    df_all_generators = df_all_generators[\r\n",
    "        df_all_generators.columns &\r\n",
    "        [\r\n",
    "            \"id\",\r\n",
    "            \"lonlat\",\r\n",
    "            \"tags.power\",\r\n",
    "            \"tags.generator:type\",\r\n",
    "            \"tags.generator:method\",\r\n",
    "            \"tags.generator:source\",\r\n",
    "            \"tags.generator:output:electricity\",\r\n",
    "            \"Type\",\r\n",
    "            \"Country\",\r\n",
    "        ]\r\n",
    "    ]\r\n",
    "    # Generate Files\r\n",
    "    outputfile_partial = os.path.join(\r\n",
    "        os.getcwd(), \"data\", \"africa_all\" + \"_generators.\"\r\n",
    "    )\r\n",
    "    df_all_generators.to_csv(outputfile_partial + \"csv\")  # Generate CSV\r\n",
    "    gdf_generators = convert_pd_to_gdf(df_all_generators)\r\n",
    "    gdf_generators.to_file(\r\n",
    "        outputfile_partial + \"geojson\", driver=\"GeoJSON\"\r\n",
    "    )  # Generate GeoJson\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overwrite Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New function\r\n",
    "\r\n",
    "def lonlat_lookup(df_way, Data):\r\n",
    "    lonlat_list = []\r\n",
    "\r\n",
    "    col = \"refs\"\r\n",
    "    if col not in df_way.columns:\r\n",
    "        print (\"refs column not found\")\r\n",
    "        df_way[col] = pd.Series().astype(float) # create empty \"refs\" if not in dataframe\r\n",
    "      \r\n",
    "    for ref in df_way[\"refs\"]:\r\n",
    "        lonlat_row = []\r\n",
    "        for r in ref:\r\n",
    "            lonlat = tuple(Data[\"Node\"][str(r)][\"lonlat\"])\r\n",
    "            lonlat_row.append(lonlat)\r\n",
    "        lonlat_list.append(lonlat_row)\r\n",
    "    return lonlat_list\r\n",
    "\r\n",
    "from shapely.geometry import Polygon\r\n",
    "\r\n",
    "def convert_ways_point(df_way, Data):\r\n",
    "    lonlat_list = lonlat_lookup(df_way, Data)\r\n",
    "    lonlat_column = []\r\n",
    "    area_column = []\r\n",
    "    for lonlat in lonlat_list:\r\n",
    "        way_polygon = Polygon(lonlat)\r\n",
    "        polygon_area = int(round(gpd.GeoSeries(way_polygon).set_crs(\"EPSG:4326\").to_crs(\"EPSG:3857\").area, -1)) # nearest tens\r\n",
    "        # print('{:g}'.format(float('{:.3g}'.format(float(polygon_area))))) # For significant numbers\r\n",
    "        area_column.append(polygon_area)\r\n",
    "        center_point = way_polygon.centroid\r\n",
    "        lonlat_column.append(list((center_point.x, center_point.y)))\r\n",
    "\r\n",
    "    # df_way.drop(\"refs\", axis=1, inplace=True, errors=\"ignore\")\r\n",
    "    df_way.insert(0, \"lonlat\", lonlat_column)\r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_substations = pd.DataFrame()\r\n",
    "df_all_lines = pd.DataFrame()\r\n",
    "df_all_generators = pd.DataFrame()\r\n",
    "test_CC = {\"NG\": \"nigeria\"}\r\n",
    "for country_code in test_CC.keys():\r\n",
    "    substation_data, line_data, generator_data = download_and_filter(country_code)\r\n",
    "    for feature in [\"substation\", \"line\",\"generator\"]:\r\n",
    "        if feature == 'substation':\r\n",
    "            df_substation = process_substation_data(country_code, substation_data)\r\n",
    "            df_all_substations = pd.concat(\r\n",
    "                [df_all_substations, df_substation])\r\n",
    "        if feature == 'line':\r\n",
    "            df_line = process_line_data(country_code, line_data)\r\n",
    "            df_all_lines = pd.concat([df_all_lines, df_line])\r\n",
    "        if feature == 'generator':\r\n",
    "            df_generator = process_generator_data(country_code, generator_data)\r\n",
    "            df_all_generators = pd.concat(\r\n",
    "                [df_all_generators, df_generator])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Substations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- SUBSTATIONS -----------\r\n",
    "\r\n",
    "# Clean\r\n",
    "df_all_substations.reset_index(drop=True, inplace=True)\r\n",
    "df_all_substations.dropna(thresh=len(df_all_substations)*0.25, axis=1, how='all', inplace = True) #Drop Columns with 75% values as N/A\r\n",
    "df_all_substations.dropna(subset=['tags.voltage'], inplace = True) # Drop any substations with Voltage = N/A\r\n",
    "df_all_substations.drop(df_all_substations.loc[df_all_substations['tags.substation']=='industrial'].index, inplace=True)\r\n",
    "df_all_substations.drop(df_all_substations.loc[df_all_substations['tags.substation']=='distribution'].index, inplace=True)\r\n",
    "\r\n",
    "# Generate Files\r\n",
    "outputfile_partial = os.path.join(os.getcwd(),'data','africa_all'+'_substations.')\r\n",
    "df_all_substations.to_csv(outputfile_partial + 'csv') # Generate CSV\r\n",
    "gdf_substations = convert_pd_to_gdf(df_all_substations.drop('refs', 1))\r\n",
    "gdf_substations.to_file(outputfile_partial+'geojson', driver=\"GeoJSON\")  # Generate GeoJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_substations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Generator -----------\r\n",
    "\r\n",
    "df_all_generators.reset_index(drop=True, inplace=True)\r\n",
    "df_all_generators.drop(columns = [\"tags.fixme\",\"tags.frequency\",\"tags.name:ar\",\"tags.building\",\"tags.barrier\"], inplace = True, errors='ignore')\r\n",
    "df_all_generators = df_all_generators[df_all_generators['tags.generator:output:electricity'].astype(str).str.contains('MW')] #removes boolean \r\n",
    "df_all_generators['tags.generator:output:electricity'] = df_all_generators['tags.generator:output:electricity'].str.extract('(\\d+)').astype(float)\r\n",
    "df_all_generators.rename(columns = {'tags.generator:output:electricity':\"power_output_MW\"}, inplace = True)\r\n",
    "# df_all_generators.dropna(thresh=len(df_all_generators)*0.25, axis=1, how='all', inplace=True) # Drop Columns with 75% values as N/A\r\n",
    "\r\n",
    "# # Generate Files\r\n",
    "# outputfile_partial = os.path.join(os.getcwd(),'data','africa_all'+'_generators.')\r\n",
    "# df_all_generators.to_csv(outputfile_partial + 'csv') # Generate CSV\r\n",
    "# gdf_generators = convert_pd_to_gdf(df_all_generators)\r\n",
    "# gdf_generators.to_file(outputfile_partial+'geojson', driver=\"GeoJSON\")  # Generate GeoJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- LINES -----------\r\n",
    "\r\n",
    "# Clean\r\n",
    "# TODO: FIX Voltage Filter\r\n",
    "# Some transmission lines carry multiple voltages, having voltage_V = 10000;20000  (two lines)\r\n",
    "# The following code keeps only the first information before the semicolon..\r\n",
    "# Needs to be corrected in future, creating two lines with the same bus ID.\r\n",
    "\r\n",
    "\r\n",
    "df_all_lines = df_all_lines[\r\n",
    "        {\r\n",
    "            \"id\",\r\n",
    "            \"refs\",\r\n",
    "            \"lonlat\",\r\n",
    "            \"tags.power\",\r\n",
    "            \"tags.cables\",\r\n",
    "            \"tags.voltage\",\r\n",
    "            \"tags.frequency\",\r\n",
    "            \"Type\",\r\n",
    "            \"Country\",\r\n",
    "        }\r\n",
    "    ]\r\n",
    "\r\n",
    "# Clean data    \r\n",
    "df_all_lines = df_all_lines.reset_index(drop=True)\r\n",
    "df_all_lines = df_all_lines.dropna(subset=['tags.voltage']) # Drop any lines with Voltage = N/A\r\n",
    "df_all_lines = df_all_lines.rename(columns = {'tags.voltage':\"voltage_V\"}) \r\n",
    "df_all_lines['voltage_V'] = df_all_lines['voltage_V'].str.split('*').str[0] #just keeps the \r\n",
    "df_all_lines['voltage_V'] = df_all_lines['voltage_V'].str.split(';').str[0]\r\n",
    "df_all_lines['voltage_V'] = df_all_lines['voltage_V'].apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna() ## if cell can't converted to float -> drop\r\n",
    "df_all_lines = df_all_lines[df_all_lines.voltage_V > 10000]\r\n",
    "# df_all_lines['end_refs'] = \r\n",
    "\r\n",
    "# Generate Files\r\n",
    "outputfile_partial = os.path.join(os.getcwd(), 'data', 'africa_all'+'_lines.')  \r\n",
    "df_all_lines.to_csv(outputfile_partial + 'csv')  # Generate CSV\r\n",
    "gdf_lines = convert_pd_to_gdf_lines(df_all_lines.drop('refs', 1), simplified=True)\r\n",
    "gdf_lines.to_file(outputfile_partial+'geojson',\r\n",
    "            driver=\"GeoJSON\")  # Generate GeoJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_line_lookup = df_all_lines[['id','refs']]\r\n",
    "display(df_line_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_node_lookup = df_all_substations[['id','refs']]\r\n",
    "df_node_lookup = df_node_lookup.dropna(subset=['refs']) # Drop any nodes with refs = N/A\r\n",
    "display(df_node_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_df(df):\r\n",
    "    out = []\r\n",
    "    for n, row in df.iterrows():\r\n",
    "        for item in row['refs']:\r\n",
    "            row['flat_ref'] = item\r\n",
    "            out += [row.copy()]\r\n",
    "\r\n",
    "    flattened_df = pd.DataFrame(out)\r\n",
    "    flattened_df.reset_index(drop=True, inplace=True)\r\n",
    "\r\n",
    "    return flattened_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "way_f = flatten_df(df_line_lookup)\r\n",
    "display(way_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_f = flatten_df(df_node_lookup)\r\n",
    "display(node_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = way_f.merge(node_f, on='flat_ref', how='inner')\r\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_line_refs = set()\r\n",
    "for ref in df_all_lines[\"refs\"]:  # goes through each row in df_way['refs']\r\n",
    "    for r in ref:\r\n",
    "        all_line_refs.add(r)\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_st_refs = set()\r\n",
    "for ref in df_all_substations.dropna(subset=['refs'])[\"refs\"]:  # goes through each row in df_way['refs']\r\n",
    "    for r in ref:\r\n",
    "        all_st_refs.add(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_line_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_line_refs.intersection(all_st_refs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64a9945f8f137ed15f7097a1dbedf9f1ce29494f0e33cb4fc5724026c999b930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('toast': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
